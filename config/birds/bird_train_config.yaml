defaults:
  - job_config
  - env/glider@train_env.env: base_singleglider
  - env/glider@test_env.env: base_singleglider
  - env/glider/air_velocity_field@train_env.env.air_velocity_field: ???
  - env/glider/air_velocity_field@test_env.env.air_velocity_field: ???
  - env/glider/aerodynamics@train_env.env.aerodynamics: ???
  - env/glider/aerodynamics@test_env.env.aerodynamics: ???
  - trainer: onpolicy
  - trainer/stop_fn: mean_reward_stop
  - model: ppo_custom_transformer
  - model/ppo_policy/dist_fn: categorical
  - model/optimizer: adam
  - logger@logger: mlflow_logger
  - override hydra/job_logging: custom_logging
  - _self_

experiment_name: bird_train

device: max_memory_cuda

trainer:
  max_epoch: 200
  step_per_epoch: 60000
  episode_per_collect: 30 # use episodes to ensure that reward estimation use full episodes
  batch_size: 2048
  repeat_per_collect: 5
  episode_per_test: 100
  stop_fn:
    stop_mean_reward: 2600
    
train_env:
  vectorized:
    count: 25
    parallel: true
  collector:
    exploration_noise: false # there is no need, PPO do this using entropy
    buffer_size: 200000
  env:
    params:
      max_sequence_length: 100
      spatial_transformation: egocentric
      egocentric_spatial_transformation:
        relative_to: last
        reverse: true
      simulation_box_params:
        box_size: [5000,5000,2000]
      initial_conditions_params:
        tangent_position_parameters:
          starting_distance_from_tangent_position_m_normal_mean: ???
          starting_distance_from_tangent_position_m_normal_sigma: ???
          starting_distance_from_tangent_position_m_normal_sigma_k: 1.
          tangent_distance_from_core_m_normal_mean: ???
          tangent_distance_from_core_m_normal_sigma: ???
          tangent_distance_from_core_m_normal_sigma_k: 1.
        altitude_earth_m_mean: 400.0
        altitude_earth_m_sigma: 50.0
        altitude_earth_m_sigma_k: 2.5
      reward_params:
        success_reward: 0.
        fail_reward: 0.
        negative_reward_enabled: true
        vertical_velocity_reward_enabled: true
        new_maximum_altitude_reward_enabled: true
      cutoff_params:
        maximum_distance_from_core_m: 500.0
        success_altitude_m: 1500.0
        fail_altitude_m: 10.0
        maximum_time_without_lift_s: 600.0
        maximum_duration_s: 3000.0
      time_params:
        dt_s: ???
        decision_dt_s: ???
      discrete_continuous_mapping:
        discrete_count: 13
        low_deg: -45.0
        high_deg: 45.0
      glider_agent_params:
        roll_control_dynamics_params:
          system:
            omega_natural_frequency: 0.2
            zeta_damping_ratio: 0.5
            k_process_gain: 1.
          control:
            proportional_gain: 15.
            integral_gain: 0.66
            derivative_gain: 40.
        air_velocity_post_process:
          velocity_noise:
            x:
              mean: 0.0
              sigma: 0.01
            y:
              mean: 0.0
              sigma: 0.01
            z:
              mean: 0.0
              sigma: 0.01

test_env:
  vectorized:
    count: 25
    parallel: true
  collector:
    exploration_noise: false
  env:
    params:
      max_sequence_length: ${train_env.env.params.max_sequence_length}
      spatial_transformation: egocentric
      egocentric_spatial_transformation:
        relative_to: last
        reverse: true
      simulation_box_params:
        box_size: [5000,5000,2000]
      initial_conditions_params:
        tangent_position_parameters:
          starting_distance_from_tangent_position_m_normal_mean: ${train_env.env.params.initial_conditions_params.tangent_position_parameters.starting_distance_from_tangent_position_m_normal_mean}
          starting_distance_from_tangent_position_m_normal_sigma: ${train_env.env.params.initial_conditions_params.tangent_position_parameters.starting_distance_from_tangent_position_m_normal_sigma}
          starting_distance_from_tangent_position_m_normal_sigma_k: 1.
          tangent_distance_from_core_m_normal_mean: ${train_env.env.params.initial_conditions_params.tangent_position_parameters.tangent_distance_from_core_m_normal_mean}
          tangent_distance_from_core_m_normal_sigma: ${train_env.env.params.initial_conditions_params.tangent_position_parameters.tangent_distance_from_core_m_normal_sigma}
          tangent_distance_from_core_m_normal_sigma_k: 1.
        altitude_earth_m_mean: 400.0
        altitude_earth_m_sigma: 50.0
        altitude_earth_m_sigma_k: 2.5
      reward_params:
        success_reward: 0.
        fail_reward: 0.
        negative_reward_enabled: true
        vertical_velocity_reward_enabled: true
        new_maximum_altitude_reward_enabled: true
      cutoff_params:
        maximum_distance_from_core_m: 500.0
        success_altitude_m: 1500.0
        fail_altitude_m: 10.0
        maximum_time_without_lift_s: 600.0
        maximum_duration_s: 3000.0
      time_params:
        dt_s: ${train_env.env.params.time_params.dt_s}
        decision_dt_s: ${train_env.env.params.time_params.decision_dt_s}
      discrete_continuous_mapping:
        discrete_count: 13
        low_deg: -45.0
        high_deg: 45.0
      glider_agent_params:
        roll_control_dynamics_params:
          system:
            omega_natural_frequency: 0.2
            zeta_damping_ratio: 0.5
            k_process_gain: 1.
          control:
            proportional_gain: 15.
            integral_gain: 0.66
            derivative_gain: 40.

model:
  transformer_net:
    input_dim: 7
    output_dim: 64
    attention_internal_dim: 64
    attention_head_num: 4
    ffnn_hidden_dim: 128
    ffnn_dropout_rate: 0.2
    max_sequence_length: ${train_env.env.params.max_sequence_length}
    embedding_dim: 64
    encoder_layer_count: 2
    enable_layer_normalization: false
    enable_causal_attention_mask: true
    is_reversed_sequence: true
    encoder_activation: relu
  actor_critic_net:
    action_space_n: 13
    actor_hidden_sizes: [128, 128]
    critic_hidden_sizes: [128, 128]
    net_output_dim: ${model.transformer_net.output_dim}
  optimizer:
    lr: 0.0003
  ppo_policy:
    discount_factor: 0.99
    deterministic_eval: true
    eps_clip: 0.2
    vf_coef: 0.5
    recompute_advantage: true
    value_clip: false
    dual_clip: 5.0
    ent_coef: 0.02
    gae_lambda: 0.9
    advantage_normalization: true
    max_batchsize: 256

hydra:
  job_logging:
    loggers:
      thermal.gaussian.make_gaussian_air_velocity_field:
        level: DEBUG
      TianshouTrainingJob:
        level: DEBUG
      LocalTensorBoardExperimentLogger:
        level: DEBUG
      utils.find_suitable_torch_device:
        level: DEBUG
      ObservationLogWrapper:
        level: DEBUG
      VectorizedObservationLogger:
        level: DEBUG
      VectorizedVideoRecorder:
        level: DEBUG
